# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: gptchat_v1.proto
# Protobuf Python Version: 4.25.0
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder

# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()

DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n\x10gptchat_v1.proto\x12\x12giga.controller.v1\"\x14\n\x12request_models_msg\"&\n\x16request_model_info_msg\x12\x0c\n\x04name\x18\x01 \x01(\t\"z\n\x10request_chat_msg\x12\x17\n\nmodel_name\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x10\n\x08messages\x18\x02 \x01(\x0c\x12\x1a\n\rchat_settings\x18\x03 \x01(\x0cH\x01\x88\x01\x01\x42\r\n\x0b_model_nameB\x10\n\x0e_chat_settings\"\"\n\x12model_response_msg\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\x0c\"\x1e\n\x0emodel_info_msg\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\x0c\"\x1e\n\x0egpt_answer_msg\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\x0c\x32\xa6\x02\n\x07GPTChat\x12^\n\nget_models\x12&.giga.controller.v1.request_models_msg\x1a&.giga.controller.v1.model_response_msg\"\x00\x12\x62\n\x0eget_model_info\x12*.giga.controller.v1.request_model_info_msg\x1a\".giga.controller.v1.model_info_msg\"\x00\x12W\n\tpost_chat\x12$.giga.controller.v1.request_chat_msg\x1a\".giga.controller.v1.gpt_answer_msg\"\x00\x62\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'gptchat_v1_pb2', _globals)
if _descriptor._USE_C_DESCRIPTORS == False:
    DESCRIPTOR._options = None
    _globals['_REQUEST_MODELS_MSG']._serialized_start = 40
    _globals['_REQUEST_MODELS_MSG']._serialized_end = 60
    _globals['_REQUEST_MODEL_INFO_MSG']._serialized_start = 62
    _globals['_REQUEST_MODEL_INFO_MSG']._serialized_end = 100
    _globals['_REQUEST_CHAT_MSG']._serialized_start = 102
    _globals['_REQUEST_CHAT_MSG']._serialized_end = 224
    _globals['_MODEL_RESPONSE_MSG']._serialized_start = 226
    _globals['_MODEL_RESPONSE_MSG']._serialized_end = 260
    _globals['_MODEL_INFO_MSG']._serialized_start = 262
    _globals['_MODEL_INFO_MSG']._serialized_end = 292
    _globals['_GPT_ANSWER_MSG']._serialized_start = 294
    _globals['_GPT_ANSWER_MSG']._serialized_end = 324
    _globals['_GPTCHAT']._serialized_start = 327
    _globals['_GPTCHAT']._serialized_end = 621
# @@protoc_insertion_point(module_scope)
